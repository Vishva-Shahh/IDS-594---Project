{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"IDS 594 MLD.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kPHBspJu2WQB","executionInfo":{"status":"ok","timestamp":1603815354731,"user_tz":300,"elapsed":23208,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"cee523b2-a8bb-4845-b3f7-bed250a8cc75","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# import os\n","# os.listdir('../input/')\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IqPZ3cJ8B1kF","executionInfo":{"status":"ok","timestamp":1603815354859,"user_tz":300,"elapsed":2957,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}}},"source":["import os\n","os.chdir('/content/drive/My Drive/IDS 576/Project/Final Project (1)')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"livMwDVeCLn2","executionInfo":{"status":"ok","timestamp":1603815356226,"user_tz":300,"elapsed":592,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"be4f8649-748f-449e-a0c9-653df787d7ad","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pwd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/IDS 576/Project/Final Project (1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"HwaJUGDM2WQF","executionInfo":{"status":"ok","timestamp":1603815362135,"user_tz":300,"elapsed":4673,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}}},"source":["from python_utils import *\n","import time\n","import matplotlib.pyplot as plt\n","import cv2 as cv\n","from math import sqrt \n","import pandas as pd\n","import numpy as np\n","from torchvision import transforms as tfs\n","import torch\n","from PIL import Image\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","import pickle\n","import random\n","import argparse\n","import sys\n","from matplotlib import pyplot as plt\n","import matplotlib.image as mpimg\n","from matplotlib.pyplot import ion, show\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZGB5ct32WQH"},"source":["time_start = time.time()\n","time_start"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bs0rLXA32WQL"},"source":["datas = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Project/fer2013/fer2013.csv\")\n","Data_RNN = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Project/fer2013/fer2013.csv\")\n","datas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOJU2x6-2WQN"},"source":["lab = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neural']\n","\n","## For RNN ## \n","\n","lab_RNN = ['This person is feeling very angry', 'This person is feeling Disgust', \n","           'This person is feeling scared', 'This person is feeling Happy',\n","           'This person is feeling sad', 'This person is extremely surprised',\n","           'This person has no emotions']\n","\n","def set_value(row_number, assigned_value): \n","    return assigned_value[row_number] \n","  \n","# Create the dictionary \n","event_dictionary = {0 : 'This person is feeling very angry', 1 : 'This person is feeling Disgust',\n","                   2 : 'This person is feeling scared', 3 : 'This person is feeling Happy',\n","                   4 : 'This person is feeling sad', 5 : 'This person is extremely surprised',\n","                   6 : 'This person has no emotions'} \n","  \n","\n","# Add a new column named 'Price' \n","Data_RNN['lab_RNN'] = Data_RNN['emotion'].apply(set_value, args =(event_dictionary, ))\n","\n","labels_num = datas.emotion.value_counts()\n","la = [0,1,2,3,4,5,6]\n","la_num = [labels_num[i] for i in range(len(labels_num))]\n","print(labels_num)\n","plt.bar(range(len(la_num)), la_num,color='rgbc',tick_label=lab) \n","for a,b in zip(la,la_num):  \n","    plt.text(a, b+0.05, '%.0f' % b, ha='center', va= 'bottom',fontsize=10)  \n","plt.show() \n","\n","Data_RNN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"czEkBA5h2WQQ"},"source":["sets = datas.Usage.value_counts()\n","da = [sets[i] for i in range(len(sets))]\n","set_la = ['Training','PublicTest','PrivateTest']\n","print(sets)\n","plt.axes(aspect=1)\n","plt.title('Size of Training,PublicTest,PrivateTest sets in the image dataset')\n","plt.pie(x = da,labels = set_la,autopct='%3.1f %%', shadow=True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOsyK8Lj2WQU","outputId":"8b979f33-7bc8-4df2-eafc-f81cdf58f6ab","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('Picture Length:',len(datas.pixels[1].split()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Picture Length: 2304\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ZLEQuX-2WQW"},"source":["time_1 = time.time()\n","print('Time to read：',round((time_1 - time_start),2),'s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Evohr9F12WQZ"},"source":["train_set = datas[(datas.Usage == 'Training')] \n","val_set = datas[(datas.Usage == 'PublicTest')]\n","test_set = datas[(datas.Usage == 'PrivateTest')] \n","X_train = np.array(list(map(str.split, train_set.pixels)), np.float32)\n","X_val = np.array(list(map(str.split, val_set.pixels)), np.float32) \n","X_test = np.array(list(map(str.split, test_set.pixels)), np.float32) \n","X_train = X_train.reshape(X_train.shape[0], 48, 48) \n","X_val = X_val.reshape(X_val.shape[0],48,48) \n","X_test = X_test.reshape(X_test.shape[0],48, 48) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QX9Mg3e2WQb"},"source":["2.Tag Data Processing"]},{"cell_type":"code","metadata":{"id":"g5Q7cit72WQb"},"source":["y_train = list(train_set.emotion) \n","y_val = list(val_set.emotion)\n","y_test = list(test_set.emotion )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOBrqSLT2WQd"},"source":["3.Checking some samples"]},{"cell_type":"code","metadata":{"id":"UMYT4IiK2WQe"},"source":["fig = plt.figure(figsize = (10,8))\n","for i in range(len(X_train[:35])):\n","    if(y_train[i] == 0 ):\n","        str_la = 'Angry'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 1):\n","        str_la = 'Disgust'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 2):\n","        str_la = 'Fear'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 3):\n","        str_la = 'Happy'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 4):\n","        str_la = 'Sad'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 5):\n","        str_la = 'Surprise'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    elif(y_train[i] == 6):\n","        str_la = 'Neural'\n","        img = Image.fromarray(np.uint8(X_train[i]))\n","    y = fig.add_subplot(5,7,i+1)\n","    y.imshow(img,cmap='gray')\n","    plt.title(str_la)\n","    y.axes.get_xaxis().set_visible(False)\n","    y.axes.get_yaxis().set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1WzYNKO2WQg"},"source":["4.Data augmentation"]},{"cell_type":"code","metadata":{"id":"j1hEdNEo2WQh"},"source":["train_preprocess = tfs.Compose([\n","    tfs.ToPILImage(),\n","    tfs.RandomCrop(44),\n","    tfs.RandomHorizontalFlip(),\n","    tfs.ToTensor(),\n","])\n","\n","\n","\n","val_preprocess = tfs.Compose([\n","    tfs.ToPILImage(),\n","    tfs.TenCrop(44),\n","    tfs.Lambda(lambda crops: torch.stack([tfs.ToTensor()(crop) for crop in crops])),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMJCmEr5FgpR","outputId":"b734bd67-5f7d-4617-dce6-8d812b44389f","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import csv\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","\n","batch_size= 128\n","class DataLr:\n","\n","    def __init__(self, data):\n","\n","        data_train = data[data.Usage == 'Training']\n","        data_validation = data[data.Usage == 'PrivateTest']\n","        data_Test = data[data.Usage == 'PublicTest']\n","\n","        self.Y_train = data_train['emotion']\n","        X_train = data_train['pixels']\n","\n","        self.Y_test = data_Test['emotion']\n","        self.X_test = data_Test['pixels']\n","        X_train = X_train.str.split(\" \", n=2304, expand=True)\n","        self.X_train = X_train.astype(np.int)\n","\n","        self.train_samples = X_train.shape[0]\n","\n","\n","class DataSetFactory:\n","\n","    def __init__(self):\n","        images = []\n","        emotions = []\n","        private_images = []\n","        private_emotions = []\n","        public_images = []\n","        public_emotions = []\n","\n","        with open(\"/content/drive/My Drive/Colab Notebooks/Project/fer2013/fer2013.csv\", 'r') as csvin:\n","            data = csv.reader(csvin)\n","            next(data)\n","            for row in data:\n","                face = [int(pixel) for pixel in row[1].split()]\n","                face = np.asarray(face).reshape(48, 48)\n","                face = face.astype('uint8')\n","\n","                if row[-1] == 'Training':\n","                    emotions.append(int(row[0]))\n","                    images.append(Image.fromarray(face))\n","                elif row[-1] == \"PrivateTest\":\n","                    private_emotions.append(int(row[0]))\n","                    private_images.append(Image.fromarray(face))\n","                elif row[-1] == \"PublicTest\":\n","                    public_emotions.append(int(row[0]))\n","                    public_images.append(Image.fromarray(face))\n","\n","        print('training size %d : private val size %d : public val size %d' % (\n","            len(images), len(private_images), len(public_images)))\n","        train_transform = transforms.Compose([transforms.ToTensor()])\n","        \n","        val_transform = transforms.Compose([transforms.ToTensor()])\n","        \n","        test_transform = transforms.Compose([transforms.ToTensor()])\n","        \n","        self.training = DataSet(transform=train_transform, images=images, emotions=emotions)\n","        self.private = DataSet(transform=val_transform, images=private_images, emotions=private_emotions)\n","        self.public = DataSet(transform=test_transform, images=public_images, emotions=public_emotions)\n","\n","class DataSet(torch.utils.data.Dataset):\n","\n","    def __init__(self, transform=None, images=None, emotions=None):\n","        self.transform = transform\n","        self.images = images\n","        self.emotions = emotions\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        emotion = self.emotions[index]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, emotion\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","\n","# DataLoaders provide various ways to get batches of examples.\n","\n","def loaders(Data, batch_size):\n","    Data = DataSetFactory()\n","    train_loader = torch.utils.data.DataLoader(Data.training, batch_size=batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(Data.private, batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(Data.public, batch_size=batch_size, shuffle=True)\n","    return train_loader, val_loader, test_loader\n","\n","\n","Data = DataSetFactory()\n","\n","train_loader, val_loader, test_loader = loaders(Data, batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training size 28709 : private val size 3589 : public val size 3589\n","training size 28709 : private val size 3589 : public val size 3589\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h06bhYTcJ461","outputId":"e8e9e467-e697-4f5a-d153-716b17963d06","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using %s for computation\" % device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using cuda for computation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xf2Il3gK2WQj","outputId":"a907e797-2ea8-4c6a-9128-c5b35ab06bdd","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["time_2 = time.time()\n","print('Data processing takes time：',round((time_2 - time_1),2),'s')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data processing takes time： 40.16 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h1QAfXpG2WQl"},"source":["Model building"]},{"cell_type":"code","metadata":{"id":"0XhOEU482WQm"},"source":["import torch\n","from torch.utils import data\n","import numpy as np\n","import torch.optim as optim\n","import torch.nn as nn\n","import pandas as pd\n","import torch.nn.functional as F\n","import torchvision.models as models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ng9Pwaso2WQn"},"source":["class Train_Dataset(data.Dataset):\n","    def __init__(self,X_train,labels):\n","        super(Train_Dataset,self).__init__()\n","        img = []\n","        label = []\n","        label = labels\n","        a = [train_preprocess(X_train[i])  for i in range(X_train.shape[0])]\n","        img = a\n","        self.img = img\n","        self.label=labels\n","      \n","            \n","    def __getitem__(self, index):\n","        \n","        imgs = self.img[index]\n","        labels = self.label[index]\n","        imgs_tensors =  imgs.type('torch.cuda.FloatTensor')\n","        return imgs_tensors, labels\n","        \n","    \n","    def __len__(self):\n","        return len(self.img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gup5B9Vn2WQp"},"source":["class Val_Dataset(data.Dataset):\n","    def __init__(self,X_val,labels):\n","        super(Val_Dataset,self).__init__()\n","        img = []\n","        label = []\n","        label = labels\n","        b = [val_preprocess(X_val[i])  for i in range(X_val.shape[0])]\n","        img = b\n","        self.img = img\n","        self.label=labels\n","      \n","             \n","    def __getitem__(self, index):\n","        \n","        imgs = self.img[index]\n","        labels = self.label[index]\n","        imgs_tensors =  imgs.type('torch.cuda.FloatTensor')\n","        return imgs_tensors, labels\n","        \n","    \n","    def __len__(self):\n","        return len(self.img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kxJGiOW2WQr"},"source":["def validate_train(model,dataset,batch_size):\n","    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n","    result,num = 0.0, 0\n","    for images,labels in val_loader:\n","        images = images.cuda()\n","        pre = model.forward(images)\n","        pre = pre.cpu()\n","        pre = np.argmax(pre.data.numpy(),axis = 1)\n","        labels = labels.data.numpy()\n","        result += np.sum((pre == labels))\n","        num += len(images)\n","    acc = result / num\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4vU6ppT2WQt"},"source":["def validate_val(model,dataset,batch_size):\n","    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n","    result,num = 0.0, 0\n","    for images,labels in val_loader:\n","        for i in range(len(images)):\n","            images[i] = images[i].cuda()\n","            pre = model.forward(images[i])\n","            pre =pre.cpu()\n","            pre = np.argmax(pre.data.numpy().mean(0))\n","            if pre == labels[i] :\n","                result = result + 1\n","        num += len(images)\n","    acc = result / num\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkBLjtm32WQv"},"source":["Multi-model fusion(vgg19 + resnet18)"]},{"cell_type":"markdown","metadata":{"id":"6rypFf562WQv"},"source":["The first layer of the fusion model, training each base model（VGG19 + resnet18）"]},{"cell_type":"code","metadata":{"id":"7dFays9E2WQw"},"source":["train_dataset = X_train\n","train_labels = y_train\n","Val_dataset = Val_Dataset( X_val,y_val)\n","Test_dataset = Val_Dataset(X_test,y_test)\n","\n","batch_size= 128\n","learning_rate = 0.001\n","epochs= 50\n","\n","#resnet18\n","resnet_historyloss = []\n","resnet_historyacc = []\n","resnet_historytrac = []\n","resnet_historytestac = []\n","\n","#vgg19\n","vgg19_historyloss = []\n","vgg19_historyacc = []\n","vgg19_historytrac = []\n","vgg19_historytestac = []\n","\n","#multiple\n","multiple_historyloss = []\n","multiple_historyacc = []\n","multiple_historytrac = []\n","multiple_historytestac = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tNWxtdmp2WQy"},"source":["batch_size = 128"]},{"cell_type":"code","metadata":{"id":"U4unFwWz2WQy"},"source":["def train_resnet18(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n","    \n","# Build model\n","     resnet18 = models.resnet18()\n","     resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","     resnet18.fc = torch.nn.Linear(in_features=512, out_features=7, bias=True)\n","    \n","     model = resnet18.cuda()\n","# Loss function\n","     loss_function = nn.CrossEntropyLoss()\n","     loss_function =  loss_function.cuda()\n","    \n","# Optimizer\n","     optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n","\n","# Training\n","     print(\"Resnet18\")\n","     for epoch in range(epochs):\n","# Each round of the input picture makes some changes to slow down the speed of overfitting\n","         Train_dataset = Train_Dataset(train_dataset,train_labels)\n","         train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n","         loss_rate = 0\n","         model.train()\n","         for images,labels in train_loader:            \n","            images = images.cuda()\n","            labels = labels.cuda()\n","            optimizer.zero_grad()\n","            output = model.forward(images)\n","            loss_rate = loss_function(output,labels)\n","            loss_rate.backward()\n","            optimizer.step()\n","         resnet_historyloss.append(loss_rate.item())\n","\n","# Print the loss per round \n","         model.eval()\n","         acc_train = validate_train(model, Train_dataset, batch_size)\n","         resnet_historytrac.append(acc_train)\n","\n","         acc_val = validate_val(model,Val_dataset,batch_size)\n","         resnet_historyacc.append(acc_val)\n","\n","         acc_test = validate_val(model,Test_dataset,batch_size)\n","         resnet_historytestac.append(acc_test)\n","        \n","         if( (epoch+1) == epochs):\n","             print(\"Resnet18 Results：\")\n","             print('The acc_train is :',acc_train)\n","             print('The acc_val is :',acc_val)\n","             print('The acc_test is :',acc_test)\n","             print('\\n')\n","\n","     \n","     print(\"Resnet18 Completed！\")        \n","     return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nD3gJSgM2WQ0"},"source":["def train_vgg19(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n"," \n","# Build model\n","     vgg19 = models.vgg19()\n","     vgg19.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","     vgg19.classifier[6] = torch.nn.Linear(in_features=4096, out_features=7, bias=True)\n","   \n","     model = vgg19.cuda()\n","# Loss function\n","     loss_function = nn.CrossEntropyLoss()\n","     loss_function =  loss_function.cuda()\n","    \n","# Optimizer\n","     optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n","\n","# Training\n","     print(\"VGG19\")\n","     for epoch in range(epochs):\n","#        Each round of the input picture makes some changes to slow down the speed of overfitting\n","         Train_dataset = Train_Dataset(train_dataset,train_labels)\n","         train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n","         loss_rate = 0\n","         model.train()\n","         for images,labels in train_loader:\n","             images = images.cuda()\n","             labels = labels.cuda()            \n","             optimizer.zero_grad()\n","             output = model.forward(images)\n","             loss_rate = loss_function(output,labels)\n","             loss_rate.backward()\n","             optimizer.step()\n","         vgg19_historyloss.append(loss_rate.item())\n","\n","\n","         model.eval()\n","         acc_train = validate_train(model, Train_dataset, batch_size)\n","         vgg19_historytrac.append(acc_train)\n","\n","         acc_val = validate_val(model,Val_dataset,batch_size)\n","         vgg19_historyacc.append(acc_val)\n","\n","         acc_test = validate_val(model,Test_dataset,batch_size)\n","         vgg19_historytestac.append(acc_test)\n","\n","\n","         if((epoch+1) == epochs):\n","            \n","             print(\"VGG19 Results：\")\n","             print('The acc_train is :',acc_train)\n","             print('The acc_val is :',acc_val)\n","             print('The acc_test is :',acc_test)\n","             print('\\n')\n","\n","     print(\"VGG19 Completed！\")       \n","     return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxbD1-Qy2WQ1","outputId":"77939dae-d2f4-453d-a9dc-10bc94ec3ead","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["resnet18 = train_resnet18(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\n","torch.save(resnet18,'fer2013_resnet18_model.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Resnet18\n","Resnet18 Results：\n","The acc_train is : 0.6853599916402522\n","The acc_val is : 0.5650599052660908\n","The acc_test is : 0.572025633881304\n","\n","\n","Resnet18 Completed！\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jtwvWePS6Vr5","outputId":"9c9c9104-b1d0-4a0f-8e3e-1c2fcde7b830","colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["vgg19 = train_vgg19(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\n","torch.save(vgg19,'fer2013_vgg19_model.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["VGG19\n","VGG19 Results：\n","The acc_train is : 0.5891880594935386\n","The acc_val is : 0.5589300640847032\n","The acc_test is : 0.566731680133742\n","\n","\n","VGG19 Completed！\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"67ypvK9A2WQ3"},"source":["Load the trained base model"]},{"cell_type":"code","metadata":{"id":"revtmKxx2WQ3","executionInfo":{"status":"ok","timestamp":1603815379703,"user_tz":300,"elapsed":11220,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}}},"source":["resnet = torch.load(\"fer2013_resnet18_model.pkl\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_fvJBdb2WQ5"},"source":["vgg = torch.load(\"fer2013_vgg19_model.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuDqtLNg2WQ8"},"source":["Build a fusion model network"]},{"cell_type":"code","metadata":{"id":"erHx49Dp2WQ8"},"source":["class Multiple(nn.Module):\n","    def __init__(self):\n","        super(Multiple,self).__init__()        \n","        \n","        self.fc = nn.Sequential(\n","             nn.Linear(in_features = 14,out_features = 7),\n","        )\n","        \n","    def forward(self,x):\n","        \n","        #After base model preprocessing\n","        result_1 = vgg(x)\n","        result_2 = resnet(x)\n","        \n","        #Features after splicing the base model\n","        result_1 = result_1.view(result_1.shape[0],-1)\n","        result_2 = result_2.view(result_2.shape[0],-1)\n","        result = torch.cat((result_1,result_2),1)\n","        \n","        #Input the features processed by the base model into the fusion model\n","        y = self.fc(result)\n","        \n","        return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjuTQq_d2WQ-"},"source":["def multiple_train(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n","\n","    \n","# Build model\n","    model = Multiple()\n","    model = model.cuda()\n","    loss_function = nn.CrossEntropyLoss()\n","    loss_function =  loss_function.cuda()\n","    optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n","\n","    print(\"Fusion model starts training！\")\n","    for epoch in range(epochs):\n","        Train_dataset = Train_Dataset(train_dataset,train_labels)\n","        train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n","        loss_rate = 0\n","        model.train()\n","        for images,labels in train_loader:         \n","            images = images.cuda()\n","            labels = labels.cuda()\n","            optimizer.zero_grad()\n","            output = model(images)\n","            loss_rate = loss_function(output,labels)\n","            loss_rate.backward()\n","            optimizer.step()\n","        multiple_historyloss.append(loss_rate.item())        \n","        \n","        model.eval()\n","        \n","        acc_train = validate_train(model, Train_dataset, batch_size)\n","        multiple_historytrac.append(acc_train)\n","        \n","        acc_val = validate_val(model,Val_dataset,batch_size)\n","        multiple_historyacc.append(acc_val)\n","        \n","        acc_test = validate_val(model,Test_dataset,batch_size)\n","        multiple_historytestac.append(acc_test)\n","\n","        \n","        print('After {} epochs : '.format(epoch+1))\n","        print('The loss_rate is :',loss_rate.item())\n","        print('The acc_train is :',acc_train)\n","        print('The acc_val is :',acc_val)\n","        print('The acc_test is :',acc_test)\n","        print('\\n')\n","    \n","    print(\"End of fusion model training！\")   \n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IruDL-6e727M"},"source":["Fusion Model Results\n"]},{"cell_type":"code","metadata":{"id":"CyL4rq7A2WRA"},"source":["model = multiple_train(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\n","torch.save(model,'fer2013_multiple_model.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ap3jZHb2WRG"},"source":["Load the base model and the fusion model separately"]},{"cell_type":"code","metadata":{"id":"c493StoiUrh6","executionInfo":{"status":"ok","timestamp":1602090638920,"user_tz":300,"elapsed":16282,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"93e982c9-c75e-4e9a-b4a8-8aefad9e6b5f","colab":{"base_uri":"https://localhost:8080/"}},"source":["vgg = torch.load('fer2013_vgg19_model.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rkJKN9cpC5nI","executionInfo":{"status":"ok","timestamp":1602090674247,"user_tz":300,"elapsed":391,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"2d438131-816f-474e-a5ba-21484d2c02bb","colab":{"base_uri":"https://localhost:8080/"}},"source":["vgg"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace=True)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace=True)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace=True)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace=True)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=7, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Ie4R7RHUUsUe","executionInfo":{"status":"ok","timestamp":1603815418566,"user_tz":300,"elapsed":592,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}}},"source":["model = torch.load('fer2013_resnet18_model.pkl')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eefl7Mg2TnT","executionInfo":{"status":"ok","timestamp":1603815425552,"user_tz":300,"elapsed":532,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"46cca65c-a2df-4024-f64e-961e1f23f27f","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=7, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"-reL6NZ17tbO","executionInfo":{"status":"error","timestamp":1603816884447,"user_tz":300,"elapsed":903,"user":{"displayName":"Vishva Vipin Shah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gin93jSJ_DOfVuwBQIXZQmHlpLnLxatAOmK-YTeiw=s64","userId":"02934471236859300555"}},"outputId":"75cba67b-4709-4404-f77f-301c9e3d7210","colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["model_mul = torch.load('fer2013_multiple_model.pkl')"],"execution_count":9,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-56cb9fa6258a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fer2013_multiple_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Multiple' on <module '__main__'>"]}]},{"cell_type":"markdown","metadata":{"id":"YeOFpNVdZQ-k"},"source":["Deploying using Flask"]},{"cell_type":"code","metadata":{"id":"sCPR2H9vMFwQ"},"source":["import pandas as pd\n","import flask\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BoCWQ4DKQlB"},"source":["\n","global graph\n","\n","graph = tf.compat.v1.get_default_graph()\n","\n","model = torch.load('fer2013_resnet18_model.pkl')\n","model.eval()\n","app = flask.Flask(__name__)\n","@app.route(\"/\", methods=[\"GET\",\"POST\"])\n","\n","def predict():\n","  data = {\"success\": False}\n","  \n","  params = flask.request.args\n","  \n","  if \"G1\" in params.keys():\n","    new_row = { \"G1\": params.get(\"G1\"), \"G2\": params.get(\"G2\"),\n","      \"G3\": params.get(\"G3\"), \"G4\": params.get(\"G4\"),\n","      \"G5\": params.get(\"G5\"), \"G6\": params.get(\"G6\"),\n","      \"G7\": params.get(\"G7\"), \"G8\": params.get(\"G8\"),\n","      \"G9\": params.get(\"G9\"), \"G10\": params.get(\"G10\") }\n","    new_x = pd.DataFrame.from_dict(new_row,\n","      orient = \"index\").transpose()\n","    \n","    with graph.as_default():\n","      data[\"response\"] = str(model.eval(new_x)[0][0])\n","      data[\"success\"] = True\n","  return flask.jsonify(data)\n","if __name__ == '__main__':\n","  app.run(host='0.0.0.0')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bw9SXwyyJjVt"},"source":["%tb"],"execution_count":null,"outputs":[]}]}